version: "3.7"
services:
  hadoop:
    build: ./images/hadoop
    container_name: hadoop
    hostname: hadoop
    ports:
      - "9870:9870" # HDFS / WebHDFS
      - "8088:8088" # ResourceManager
      - "9000:9000" # Conexao Externa HDFS
      - "8042:8042" # DataNode
      - "9864:9864" # DataNode WebUI
      - "8888:8888" #Jupyter
      - "10000:10000" # HiveServer2 - Binary Mode
      - "10001:10001" # HiveServer2 - HTTP Mode
      - "10002:10002" # Hiveserver2 WebUI

    volumes:
      - hadoop_namenode:/usr/hadoop/data/namenode
      - hadoop_datanode:/usr/hadoop/data/datanode
      - ./notebooks:/usr/notebooks
      - jupyterlab_config:/home/jovyan/.local/share/jupyter
      - jupyterlab_config:/home/jovyan/.jupyter
    networks:
      - danilake
    command: ["/usr/local/bin/bootstrap.sh"]

  spark-master:
    build: ./images/hadoop
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"  # Porta do Spark Master
      - "8080:8080"  # Web UI do Spark Master
    networks:
      - danilake
    command: ["/usr/local/bin/start-master.sh"]
    depends_on:
      - hadoop   

  spark-worker:
    build: ./images/hadoop
    container_name: spark-worker
    hostname: spark-worker
    ports:
      - "8081:8081"  # Web UI do Spark Worker
    networks:
      - danilake
    command: ["/usr/local/bin/start-worker.sh"]
    depends_on:
      - hadoop 

  airflow:
    build: ./images/airflow
    container_name: airflow
    hostname: airflow
    ports:
      - "8099:8099"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./notebooks:/opt/airflow/notebooks
    networks:
      - danilake
    depends_on:
      - spark-master
      - spark-worker

networks:
  danilake:
    driver: bridge

volumes:
  jupyterlab_config:
  hadoop_namenode:
  hadoop_datanode: