version: "3.7"
services:
  hadoop:
    build: ./images/hadoop
    container_name: hadoop
    hostname: hadoop
    ports:
      - "9870:9870" # HDFS / WebHDFS
      - "8088:8088" # ResourceManager
      - "9000:9000" # Conexao Externa HDFS
      - "8042:8042" # DataNode
      - "9864:9864" # DataNode WebUI
      - "10000:10000" # HiveServer2 - Binary Mode
      - "10001:10001" # HiveServer2 - HTTP Mode
      - "10002:10002" # Hiveserver2 WebUI
      - "8099:8099" # Airflow
      - "4040:4040" # Spark jobs UI

    volumes:
      - hadoop_namenode:/usr/hadoop/data/namenode
      - hadoop_datanode:/usr/hadoop/data/datanode
      - ./notebooks:/usr/notebooks
      - jupyterlab_config:/home/jovyan/.local/share/jupyter
      - jupyterlab_config:/home/jovyan/.jupyter
      - jupyterlab_config:/root/.jupyter/
      - ./dags:/opt/airflow/dags
      - spark_jars:/usr/spark/jars
    networks:
      - danilake
    command: ["/usr/local/bin/bootstrap.sh"]

  spark-master:
    build: ./images/hadoop
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"  # Porta do Spark Master
      - "8080:8080"  # Web UI do Spark Master
    volumes:
      - spark_jars:/usr/spark/jars
    networks:
      - danilake
    command: ["/usr/local/bin/start-master.sh"]
    depends_on:
      - hadoop   

  spark-worker:
    build: ./images/hadoop
    container_name: spark-worker
    hostname: spark-worker
    ports:
      - "8081:8081"  # Web UI do Spark Worker
    volumes:
      - spark_jars:/usr/spark/jars
    networks:
      - danilake
    command: ["/usr/local/bin/start-worker.sh"]
    depends_on:
      - hadoop 

  mlflow:
    build: ./images/mlflow
    container_name: mlflow
    hostname: mlflow
    ports:
      - "5000:5000"
    networks:
      - danilake
    volumes:
      - mlflow_runs:/app/mlruns 
      - ./notebooks:/usr/notebooks

  sqlserver:
    build: ./images/sql
    container_name: sqlserver
    privileged: true
    hostname: sqlserver
    ports:
      - "1433:1433"
      - "2222:22"    # Porta para o SSH
      - "53682:53682"    # rclone
    networks:
      - danilake
    volumes:
      - sqlserver_data:/var/opt/mssql  
      - ./notebooks:/usr/notebooks
    environment:
      MSSQL_TCP_PORT: 1433 

  jupyter:
    container_name: jupyter
    build: ./images/jupyter
    hostname: jupyter
    ports:
      - 8888:8888
    tty: true
    volumes:
      - ./notebooks:/notebooks
      - jupyterlab_config:/root/.jupyter/
      - spark_jars:/usr/spark/jars
    networks:
      - danilake
    command: bash -c "/start.sh"
      
networks:
  danilake:
    driver: bridge

volumes:
  jupyterlab_config:
  hadoop_namenode:
  hadoop_datanode:
  sqlserver_data:
  mlflow_runs:
  spark_jars: