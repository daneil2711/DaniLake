{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224c8529-0f51-49b6-bdfa-a4f15cdb7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.0 MB 139 kB/s  eta 0:00:01    |██▍                             | 23.3 MB 12.4 MB/s eta 0:00:24     |██████████▉                     | 107.3 MB 13.2 MB/s eta 0:00:16     |██████████████▎                 | 141.1 MB 12.8 MB/s eta 0:00:14     |███████████████▏                | 150.2 MB 11.0 MB/s eta 0:00:16\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488513 sha256=ed0ddfc3cfd0a25f816359721322e658272834cada3ddb1807a7dee0f8abd531\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/09/11/aa01d01a7f005fda8a66ad71d2be7f8aa341bddafb27eee3c7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ec815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-269df370-ca17-4497-a2d7-0dedac52aeb4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.4.0/delta-core_2.12-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.4.0!delta-core_2.12.jar (1253ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.4.0/delta-storage-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.4.0!delta-storage.jar (237ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (271ms)\n",
      ":: resolution report :: resolve 4233ms :: artifacts dl 1766ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-269df370-ca17-4497-a2d7-0dedac52aeb4\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (4537kB/10ms)\n",
      "24/06/06 02:00:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feadfbcf8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform \n",
    "from pyspark.sql import SparkSession\n",
    "    \n",
    "if platform.system() == 'Windows':\n",
    "    path = 'C:'\n",
    "    spark = (SparkSession.builder.master(\"local[*]\")\n",
    "        .appName(\"TesteHive\")\n",
    "        .config(\"hive.metastore.uris\", \"thrift://192.168.15.4:9083\")\n",
    "        .config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/users/hive/warehouse\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate())\n",
    "else:\n",
    "    try:\n",
    "        spark.stop()\n",
    "    except:\n",
    "        pass\n",
    "    path = '/mnt/c'\n",
    "    spark = (SparkSession.builder.appName(\"MyApp\")\n",
    "            .config(\"hive.metastore.uris\", \"thrift://192.168.15.4:9083\")\n",
    "            .config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/users/hive/warehouse\")\n",
    "            .config(\"spark.jars.packages\",\"io.delta:delta-core_2.12:2.4.0\")\n",
    "            .config(\"spark.driver.memory\",\"2g\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate())\n",
    "    \n",
    "    # spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc59506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bcfc3f-a03f-4013-a34c-712ce8813539",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /usr/notebooks/Validacoes.ipynb /user/Daniel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d57898e-0816-4b2f-8bf1-c12840fd1bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root root      54140 2024-06-23 17:39 /user/Daniel/Validacoes.ipynb\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/Daniel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d1db53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.wm.default.pool.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.preempt.independent does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.output.format.arrow does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.tez.llap.min.reducer.per.executor does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.arrow.root.allocator.limit does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.use.checked.expressions does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.mapjoin does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.complex.types.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.wm.worker.threads does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.partitions.dump.parallelism does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.uri.selection does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.strict.checks.no.partition.filter does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.dpp.factor does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.stats.filter.in.min.ratio does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.client.cache.initial.capacity does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.stats.ndv.estimate.percent does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.methods does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.optimize.joinreducededuplication does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.client.cache.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.stats.fetch.bitvector does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.disable.unsafe.external.table.operations does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.incremental does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.materializedviews.registry.impl does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.exec.orc.delta.streaming.optimizations.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.stats.ndv.algo does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.spark.job.max.tasks does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.msck.repair.batch.max.retries does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.prewarm.spark.timeout does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde.list does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.plugin.client.num.threads does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.test.bucketcodec.version does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.reexecution.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.time.window does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.batch.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.headers does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.join.inner.residual does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.enable does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.trace.always.dump does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.persist.scope does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.mm.allow.originals does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.compactor.compact.insert.only does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.txn.xlock.iow does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.spark.rsc.conf.list does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.cache.defaultfs.only.native.fileid does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.spark.optimize.shuffle.serde does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.testing.remove.logs does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.distcp.privileged.doAs does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.strict.checks.orderby.no.limit does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.client.cache.expiry.time does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.allocator.defrag.headroom does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.notification.event.consumers does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.input.format.supports.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.client.cache.max.capacity does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.dumpdir.clean.freq does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.spark.use.ts.stats.for.mapjoin does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.dump.include.acid.tables does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.webui.use.pam does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.reexecution.max.count does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.share.object.pools does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.service.metrics.codahale.reporter.classes does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.tez.session.events.print.summary does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.base does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.mm.avoid.s3.globstatus does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.replica.functions.root.dir does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.lifetime does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.thrift.http.compression.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.execution.ptf.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.optimize.shared.work.extended does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.row.identifier.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.reexecution.always.collect.operator.stats does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.dumpdir.ttl does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.local.time.zone does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.tez.wm.am.registry.timeout does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.registry.namespace does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.create.as.insert.only does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.oversubscribe.factor does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.arrow.batch.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.retry.sleep.interval does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.approx.max.load.tasks does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.results.cache.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.legacy.schema.for.all.serdes does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.tez.dag.status.check.interval does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.druid.bitmap.type does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.spark.dynamic.partition.pruning.map.join.only does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.memory.oversubscription.max.executors.per.query does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.trace.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.plugin.rpc.num.handlers does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.wm.allow.any.pool.via.jdbc does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.groupby.complex.types.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.avro.timestamp.skip.conversion does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.results.cache.nontransactional.tables.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.stats.correlated.multi.key.joins does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.db.type does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.check.interval.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.zookeeper.connection.timeout does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.reexecution.strategies does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user.ipaddress does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.monitor.check.interval does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.optimize.shared.work does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.stats.estimate does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.allocator.discard.method does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.tez.cartesian-product.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.max.retries does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.heap.memory.monitor.usage.threshold does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.privilege.synchronizer.interval does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.adaptor.suppress.evaluate.exceptions does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.materializedview.rebuild.incremental does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.spark.stage.max.tasks does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.testing.short.logs does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.spark.explain.user does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.describe.partitionedtable.ignore.stats does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.operation.log.cleanup.delay does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.dump.metadata.only does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.optimize.countdistinct does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.auto.convert.join.shuffle.max.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.plugin.acl does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.schema.info.class does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.tez.queue.access.check does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.external.splits.temp.table.storage.format does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.row.wrapper.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.constraint.notnull.enforce does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.cli.print.escape.crlf does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.trigger.validation.interval does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.origins does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.ipaddress does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.external.splits.order.by.force.single.split does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.metastore.client.cache.stats.enabled does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.notification.event.poll.interval does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.transactional.concatenate.noblock does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.strategy does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.if.expr.mode does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.exim.test.mode does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.results.cache.directory does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.results.cache.wait.for.pending.results does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.remove.orderby.in.subquery does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.tez.bmj.use.subcache does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.min does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.wm.pool.metrics does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.add.raw.reserved.namespace does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.resource.use.hdfs.location does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.stats.num.nulls.estimate.percent does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.acid does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.zk.sm.session.timeout does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.ptf.max.memory.buffering.batch.count does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.am.registry does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.druid.overlord.address.default does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.optimize.remove.sq_count_check does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.server2.webui.enable.cors does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.vectorized.row.serde.inputformat.excludes does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.combine.equivalent.work.optimization does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.lock.query.string.max.length does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.llap.io.track.cache.usage does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.use.orc.codec.pool does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.query.results.cache.max.size does not exist\n",
      "24/06/06 02:00:39 WARN HiveConf: HiveConf of name hive.repl.bootstrap.dump.open.txn.timeout does not exist\n",
      "24/06/06 02:01:00 WARN metastore: Failed to connect to the MetaStore Server...\n",
      "24/06/06 02:01:22 WARN metastore: Failed to connect to the MetaStore Server...\n",
      "24/06/06 02:01:44 WARN metastore: Failed to connect to the MetaStore Server...\n",
      "24/06/06 02:01:45 WARN HiveClientImpl: HiveClient got thrift exception, destroying client and retrying (0 tries remaining)\n",
      "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1567)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listDatabases(SessionCatalog.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.listNamespaces(V2SessionCatalog.scala:267)\n",
      "\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.listNamespaces(DelegatingCatalogExtension.java:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)\n",
      "\t... 66 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\n",
      "\t... 73 more\n",
      "Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:406)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:303)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:406)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listDatabases(SessionCatalog.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.listNamespaces(V2SessionCatalog.scala:267)\n",
      "\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.listNamespaces(DelegatingCatalogExtension.java:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.ShowNamespacesExec.run(ShowNamespacesExec.scala:42)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:607)\n",
      "\tat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\n",
      "\t... 81 more\n",
      ")\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:527)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\t... 78 more\n",
      "24/06/06 02:01:46 WARN HiveClientImpl: Deadline exceeded\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mshow databases\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "show databases\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5181e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+--------+\n",
      "|count(DISTINCT N_AIH, DT_SAIDA, IDENT)|count(1)|\n",
      "+--------------------------------------+--------+\n",
      "|                                299648|  299648|\n",
      "+--------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "select \n",
    "\n",
    "--substring(UF_zi,1,2) as UF,\n",
    "--count (distinct ano_cmpt),\n",
    "--count (distinct mes_cmpt),\n",
    "count (distinct N_AIH,DT_SAIDA,IDENT),\n",
    "count(*)\n",
    "\n",
    "from bronze.rd_sih\n",
    "\n",
    "--group by 1\n",
    "--order by 1\n",
    "''').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa04f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+--------+\n",
      "|count(DISTINCT N_AIH, DT_SAIDA, IDENT)|count(1)|\n",
      "+--------------------------------------+--------+\n",
      "|                                299648|  345124|\n",
      "+--------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/DataSus/rd/csv',sep=\";\",header=True)\n",
    "df.createOrReplaceTempView(\"test\")\n",
    "\n",
    "spark.sql('''\n",
    "\n",
    "select \n",
    "\n",
    "--substring(UF_zi,1,2) as UF,\n",
    "--count (distinct ano_cmpt),\n",
    "--count (distinct mes_cmpt),\n",
    "count (distinct N_AIH,DT_SAIDA,IDENT),\n",
    "count(*)\n",
    "\n",
    "from test\n",
    "\n",
    "--group by 1\n",
    "--order by 1\n",
    "''').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "098f6676-ca1d-4c11-93d1-3e954b0dacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
    "                   'mask': ['red', 'purple'],\n",
    "                   'weapon': ['sai', 'bo staff']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ecabc18-829b-4327-99f3-4eda65ef7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8694ed9-8da4-4090-832c-01b76da7b61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
