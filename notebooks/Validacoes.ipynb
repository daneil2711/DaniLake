{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ec815b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplatform\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMyApp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.warehouse.dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://localhost:9000/users/hive/warehouse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhive.metastore.uris\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthrift://localhost:9083\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menableHiveSupport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# spark = configure_spark_with_delta_pip(builder).getOrCreate()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m spark\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import platform \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"MyApp\")\n",
    "        .config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/users/hive/warehouse\")\n",
    "        .config(\"spark.driver.memory\",\"2g\")\n",
    "        .config(\"hive.metastore.uris\", \"thrift://localhost:9083\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate())\n",
    "    \n",
    "    # spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a511bafc-335d-41c4-9e4b-c9c01a2bbcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc59506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45bcfc3f-a03f-4013-a34c-712ce8813539",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /usr/notebooks/Farmacias.csv /user/Daniel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d57898e-0816-4b2f-8bf1-c12840fd1bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 Daniel root        758 2024-06-26 02:05 /user/Daniel/Farmacias.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/Daniel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9edf08b-a3ae-429d-9e40-89a956e7fe14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create database silver\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d1db53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------------------+---------------+\n",
      "|              CIDADE|FLAG_SHOPPING|SEGUNDO_PRINCIPAL_SETOR|FATURAMENTO_TRI|\n",
      "+--------------------+-------------+-----------------------+---------------+\n",
      "|           São Paulo|          Sim|             Cosméticos|           34,8|\n",
      "|            Campinas|          Sim|             Cosméticos|           28,6|\n",
      "|           Guarulhos|          Sim|        Higiene pessoal|           27,3|\n",
      "|              Osasco|          Sim|             Perfumaria|           24,5|\n",
      "|São Bernardo do C...|          Sim|             Cosméticos|           24,1|\n",
      "|      Ribeirão Preto|          Sim|             Cosméticos|           23,6|\n",
      "|             Diadema|          Não|             Perfumaria|           21,8|\n",
      "|            Sorocaba|          Sim|             Cosméticos|           19,8|\n",
      "|        Praia Grande|          Não|        Higiene pessoal|           19,4|\n",
      "|             Jundiaí|          Não|             Perfumaria|           18,5|\n",
      "|         Santo André|          Sim|        Higiene pessoal|           17,6|\n",
      "|              Santos|          Não|        Higiene pessoal|           16,2|\n",
      "|          Piracicaba|          Sim|             Perfumaria|           14,9|\n",
      "|São José do Rio P...|          Não|             Cosméticos|           13,4|\n",
      "|             Limeira|          Sim|             Perfumaria|           13,2|\n",
      "|     Mogi das Cruzes|          Sim|        Higiene pessoal|           12,6|\n",
      "|              Suzano|          Não|             Perfumaria|            8,9|\n",
      "|           Americana|          Não|        Higiene pessoal|            6,3|\n",
      "|              Franca|          Não|             Perfumaria|            2,1|\n",
      "|                null|         null|                   null|           null|\n",
      "+--------------------+-------------+-----------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/user/Daniel/Farmacias.csv\",sep=\";\",header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5181e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+--------+\n",
      "|count(DISTINCT N_AIH, DT_SAIDA, IDENT)|count(1)|\n",
      "+--------------------------------------+--------+\n",
      "|                                299648|  299648|\n",
      "+--------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "\n",
    "select \n",
    "\n",
    "--substring(UF_zi,1,2) as UF,\n",
    "--count (distinct ano_cmpt),\n",
    "--count (distinct mes_cmpt),\n",
    "count (distinct N_AIH,DT_SAIDA,IDENT),\n",
    "count(*)\n",
    "\n",
    "from bronze.rd_sih\n",
    "\n",
    "--group by 1\n",
    "--order by 1\n",
    "''').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa04f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+--------+\n",
      "|count(DISTINCT N_AIH, DT_SAIDA, IDENT)|count(1)|\n",
      "+--------------------------------------+--------+\n",
      "|                                299648|  345124|\n",
      "+--------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/DataSus/rd/csv',sep=\";\",header=True)\n",
    "df.createOrReplaceTempView(\"test\")\n",
    "\n",
    "spark.sql('''\n",
    "\n",
    "select \n",
    "\n",
    "--substring(UF_zi,1,2) as UF,\n",
    "--count (distinct ano_cmpt),\n",
    "--count (distinct mes_cmpt),\n",
    "count (distinct N_AIH,DT_SAIDA,IDENT),\n",
    "count(*)\n",
    "\n",
    "from test\n",
    "\n",
    "--group by 1\n",
    "--order by 1\n",
    "''').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "098f6676-ca1d-4c11-93d1-3e954b0dacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
    "                   'mask': ['red', 'purple'],\n",
    "                   'weapon': ['sai', 'bo staff']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ecabc18-829b-4327-99f3-4eda65ef7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8694ed9-8da4-4090-832c-01b76da7b61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
